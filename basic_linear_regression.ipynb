{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae6ceb7-e252-47ec-9cd4-2202b408e5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "import utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a2ac2c-d901-4bca-a478-ebe98fb342cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = utils.load_csv_data('<local-path>/data/Advertising.csv')\n",
    "\n",
    "feature_keys = ['tv', 'radio', 'newspaper']\n",
    "target_key = 'sales'\n",
    "\n",
    "X, y = utils.prepare_data(raw_data, feature_keys, target_key)\n",
    "\n",
    "print(\"X Shape: \", X.shape)\n",
    "print(\"X length: \", len(X))\n",
    "print(\"X first 5 features: \", X[:5])\n",
    "print(\"X type: \", type(X))\n",
    "\n",
    "print(\"y Shape: \", y.shape)\n",
    "print(\"y length: \", len(y))\n",
    "print(\"y first 5 features: \", y[:5])\n",
    "print(\"y type: \", type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658e2a6f-7e3a-4207-bc85-f1e0f234050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 5 features vs target\n",
    "print(\"Plot first 5 X vs y\")\n",
    "utils.plot_features_vs_target(X[:5], y[:5], feature_keys, target_key)\n",
    "\n",
    "# Plot the entire features vs target\n",
    "print(\"Plot entire X vs y\")\n",
    "utils.plot_features_vs_target(X, y, feature_keys, target_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda76f85-0ab4-4615-a27b-0b69292efe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split the dataset into training, validation, and test sets\n",
    "# First split: 75% training, 25% temporary set\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.25, random_state=55)\n",
    "# Second split: Divide the temporary set into validation and test sets (50% each, which is 12.5% of the original data each)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=55)\n",
    "\n",
    "# Step 2: Normalize using TensorFlow (adapt on train only)\n",
    "# Create a normalization layer that will standardize the features\n",
    "normalizer = tf.keras.layers.Normalization(axis=-1)\n",
    "# Fit the normalizer only on training data to avoid data leakage\n",
    "normalizer.adapt(X_train)  # Only fit on training data\n",
    "\n",
    "# Step 3: Transform all datasets using the fitted normalizer\n",
    "# Apply normalization to training data and convert to numpy array\n",
    "X_train_norm = normalizer(X_train).numpy()\n",
    "# Apply same normalization to validation data\n",
    "X_val_norm = normalizer(X_val).numpy() \n",
    "# Apply same normalization to test data\n",
    "X_test_norm = normalizer(X_test).numpy()\n",
    "\n",
    "# Print shapes of all datasets to verify the splitting worked correctly\n",
    "print(\"X_train_norm Shape: \", X_train_norm.shape)\n",
    "print(\"y_train Shape: \", y_train.shape)\n",
    "print(\"X_val_norm Shape: \", X_val_norm.shape)\n",
    "print(\"y_val Shape: \", y_val.shape)\n",
    "print(\"X_test_norm Shape: \", X_test_norm.shape)\n",
    "print(\"y_test Shape: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af239a6-30b9-444c-95d4-aae05a6696b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(\n",
    "    X: np.ndarray, \n",
    "    W: np.ndarray, \n",
    "    b: float\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Predict target values using linear regression.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Feature matrix of shape (n_samples, n_features)\n",
    "        W (np.ndarray): Weight vector of shape (n_features,)\n",
    "        b (float): Bias term\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Predicted values of shape (n_samples,)\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate predictions using the linear regression formula: f(x) = XÂ·W + b\n",
    "    # - np.dot(X, W) computes the matrix multiplication between features and weights\n",
    "    # - Adding b applies the bias term to each prediction\n",
    "    f_x = np.dot(X, W) + b\n",
    "\n",
    "    return f_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a412fadd-4e23-4feb-a978-b5d1b03f66a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_loss(\n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray, \n",
    "    W: np.ndarray, \n",
    "    b: float\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute the mean squared error loss.\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): Feature matrix of shape (m, n)\n",
    "        y (np.ndarray): Target vector of shape (m,)\n",
    "        W (np.ndarray): Weight vector of shape (n,)\n",
    "        b (float): Bias term\n",
    "\n",
    "    Returns:\n",
    "        float: Mean squared error loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the number of training examples\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Calculate predictions using the predict function\n",
    "    predictions = predict(X, W, b) \n",
    "    \n",
    "    # Compute the squared differences between predictions and actual values\n",
    "    squared_errors = (predictions - y) ** 2\n",
    "    \n",
    "    # Calculate the mean squared error loss with the 1/2m factor\n",
    "    loss = np.sum(squared_errors) / (2 * m)\n",
    "    \n",
    "    return loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dafd3d5-53b1-45c7-aa34-631abaf60448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(\n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray, \n",
    "    W: np.ndarray, \n",
    "    b: float\n",
    ") -> tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Compute the gradient of the cost function with respect to parameters W and b.\n",
    "    \n",
    "    Args:\n",
    "        X: Input features, shape (m, n) where m is number of examples and n is number of features\n",
    "        y: Target values, shape (m,)\n",
    "        W: Weight parameters, shape (n,)\n",
    "        b: Bias parameter\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Gradients with respect to W and b\n",
    "            - d_dw: Gradient with respect to W, shape (n,)\n",
    "            - d_db: Gradient with respect to b, scalar\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the number of examples (m) and features (n)\n",
    "    m, n = X.shape\n",
    "\n",
    "    # Calculate model predictions using current parameters\n",
    "    predictions = predict(X, W, b) \n",
    "    # Compute the error (difference between predictions and actual values)\n",
    "    error = predictions - y\n",
    "\n",
    "    # Calculate gradient for weights by taking dot product of X transpose and error, then normalize by m\n",
    "    # This is the partial derivative of the cost function with respect to W\n",
    "    d_dw = np.dot(X.T, error) / m\n",
    "    \n",
    "    # Calculate gradient for bias by summing all errors and normalizing by m\n",
    "    # This is the partial derivative of the cost function with respect to b\n",
    "    d_db = np.sum(error) / m\n",
    "\n",
    "    return d_dw, d_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7391b3-3bb0-4efc-81f3-ff821c750161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(\n",
    "    X: np.ndarray, \n",
    "    y: np.ndarray, \n",
    "    W: np.ndarray, \n",
    "    b: float, \n",
    "    learning_rate: float\n",
    ")-> tuple[np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    Perform one step of gradient descent to update model parameters.\n",
    "    \n",
    "    Args:\n",
    "        X: Input features, shape (m, n) where m is number of examples and n is number of features\n",
    "        y: Target values, shape (m,) or (m, 1)\n",
    "        W: Current weight parameters, shape (n,) or (n, 1)\n",
    "        b: Current bias parameter\n",
    "        learning_rate: Step size for the gradient descent update\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Updated weights W and bias b after one step of gradient descent\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate gradients for weights and bias using current parameters\n",
    "    dW, db = compute_gradient(X, y, W, b)\n",
    "\n",
    "    # Update weights by subtracting the learning rate multiplied by the gradient\n",
    "    W =  W - (learning_rate * dW)\n",
    "    # Update bias by subtracting the learning rate multiplied by the gradient\n",
    "    b = b - (learning_rate * db)\n",
    "\n",
    "    return W, b   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f16c9f1-6e15-4bac-920e-b835ca40a5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    X_train: np.ndarray, \n",
    "    y_train: np.ndarray, \n",
    "    X_val: np.ndarray,\n",
    "    y_val: np.ndarray,\n",
    "    W: np.ndarray, \n",
    "    b: float, \n",
    "    learning_rate: float, \n",
    "    epochs: int\n",
    ") -> tuple[np.ndarray, float, list[float], list[float]]:\n",
    "    \"\"\"\n",
    "    Train a linear model using gradient descent optimization.\n",
    "    \n",
    "    Args:\n",
    "        X_train: Training features, shape (n_samples, n_features)\n",
    "        y_train: Training target values, shape (n_samples,)\n",
    "        X_val: Validation features, shape (n_samples, n_features)\n",
    "        y_val: Validation target values, shape (n_samples,)\n",
    "        W: Initial weight matrix, shape (n_features,)\n",
    "        b: Initial bias term\n",
    "        learning_rate: Step size for gradient descent updates\n",
    "        epochs: Number of training iterations\n",
    "        \n",
    "    Returns:\n",
    "        tuple: Updated weights W, bias b, train loss history, and validation loss history after training\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize empty lists to store loss values during training\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "\n",
    "    # Iterate through the specified number of training epochs\n",
    "    for epoch in range(epochs):\n",
    "        # Calculate and store the mean squared loss on training data\n",
    "        train_loss = mean_squared_loss(X_train, y_train, W, b)\n",
    "        train_loss_history.append(train_loss)\n",
    "\n",
    "        # Calculate and store the mean squared loss on validation data\n",
    "        val_loss = mean_squared_loss(X_val, y_val, W, b)\n",
    "        val_loss_history.append(val_loss)      \n",
    "\n",
    "        # Print progress every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
    "\n",
    "        # Update model parameters (weights and bias) using gradient descent\n",
    "        W, b = gradient_descent(X_train, y_train, W, b, learning_rate)\n",
    "\n",
    "    # Return the trained model parameters and loss histories\n",
    "    return W, b, train_loss_history, val_loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d137b83c-73dd-4b4c-bc20-8e155a2519e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights and bias\n",
    "W = np.zeros(X_train.shape[1])  # shape (n_features,)\n",
    "b = 0.0\n",
    "learning_rate = 0.01  # Step size for gradient descent\n",
    "epochs = 1000  # Number of training iterations\n",
    "\n",
    "# Train the linear regression model\n",
    "W_trained, b_trained, train_losses, val_losses = train(X_train_norm, y_train, X_val_norm, y_val, W, b, learning_rate, epochs)\n",
    "\n",
    "# Print the final trained parameters\n",
    "print(f\"Training parameters Weight: {W_trained}, bias {b_trained}\")\n",
    "\n",
    "# Visualize the training and validation loss over epochs\n",
    "# This helps to monitor model convergence and potential overfitting\n",
    "utils.plot_loss_curve(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cdfff7-e243-4561-bc22-26edb546294a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test set using trained weights and bias\n",
    "y_predict = predict(X_test_norm, W_trained, b_trained)\n",
    "\n",
    "# Calculate the mean squared loss on the test set\n",
    "y_predict_loss = mean_squared_loss(X_test_norm, y_test, W_trained, b_trained)   \n",
    "\n",
    "# Print the test loss\n",
    "print(f\"Test Loss: {y_predict_loss}\")\n",
    "\n",
    "# Create a plot comparing actual vs predicted sales\n",
    "utils.plot_predictions(y_test, y_predict, 'Predicted vs Actual Sales', 'Actual Sales', 'Predicted Sales')\n",
    "\n",
    "# Print the first 25 actual and predicted values for comparison\n",
    "for i in range(25):\n",
    "    print(\"Print actual va predicted values\")\n",
    "    print(f\"Actual: {y_test[i]}, Predicted: {y_predict[i]:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb6a2fd-6277-4341-9cae-e9b8ebdafea8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (johnenv1)",
   "language": "python",
   "name": "johnenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
